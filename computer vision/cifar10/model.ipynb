{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18raj\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# img, label = train_dataset[0], train_dataset[1]\n",
    "img, label = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18raj\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\matplotlib\\text.py:1215: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img[1])\n",
    "plt.title(label[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (10000, 32, 32, 3) (40000, 1) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.length = len(X)\n",
    "        self.X = X/255.0\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def transformHorizontal(self, img):\n",
    "        for cni, cn in enumerate(img):\n",
    "            for rowi, row in enumerate(img[cni]):\n",
    "                img[cni][rowi] = np.flip(row)\n",
    "        return img\n",
    "    def transformVertical(self, img):\n",
    "        for cni, cn in enumerate(img):\n",
    "            img[cni] = np.flip(cn)\n",
    "        return img\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.moveaxis(self.X[idx], 2, 0)\n",
    "        y = self.y[idx].squeeze()\n",
    "        rnd = random.randint(0, 5)\n",
    "        if rnd == 0:\n",
    "            X = self.transformHorizontal(X)\n",
    "        elif rnd == 1:\n",
    "            X = self.transformVertical(X)\n",
    "        X_tensor = torch.from_numpy(X).to(torch.float32)\n",
    "        y_tensor = torch.tensor(y).to(torch.long)\n",
    "        return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(X_train, y_train)\n",
    "val_data = MyDataset(X_val, y_val)\n",
    "test_data = MyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.5804, 0.5725, 0.5843,  ..., 0.5608, 0.5569, 0.5608],\n",
       "           [0.5725, 0.5647, 0.5020,  ..., 0.5216, 0.5216, 0.5255],\n",
       "           [0.5686, 0.4588, 0.2000,  ..., 0.4902, 0.4863, 0.4941],\n",
       "           ...,\n",
       "           [0.4549, 0.4431, 0.4392,  ..., 0.3490, 0.3529, 0.3647],\n",
       "           [0.4588, 0.4431, 0.4392,  ..., 0.3529, 0.3569, 0.3647],\n",
       "           [0.4196, 0.4078, 0.4039,  ..., 0.3647, 0.3686, 0.3765]],\n",
       " \n",
       "          [[0.7137, 0.7020, 0.7137,  ..., 0.7333, 0.7294, 0.7373],\n",
       "           [0.7059, 0.6941, 0.6196,  ..., 0.7020, 0.7020, 0.7098],\n",
       "           [0.7216, 0.5725, 0.2706,  ..., 0.6824, 0.6784, 0.6863],\n",
       "           ...,\n",
       "           [0.6824, 0.6627, 0.6588,  ..., 0.5569, 0.5608, 0.5765],\n",
       "           [0.6824, 0.6627, 0.6588,  ..., 0.5608, 0.5647, 0.5765],\n",
       "           [0.6275, 0.6118, 0.6078,  ..., 0.5804, 0.5843, 0.5922]],\n",
       " \n",
       "          [[0.4078, 0.3882, 0.3961,  ..., 0.3804, 0.3804, 0.3804],\n",
       "           [0.3922, 0.3961, 0.3843,  ..., 0.3373, 0.3373, 0.3412],\n",
       "           [0.3725, 0.3255, 0.1961,  ..., 0.3137, 0.3137, 0.3176],\n",
       "           ...,\n",
       "           [0.1922, 0.1922, 0.1922,  ..., 0.2000, 0.2039, 0.2118],\n",
       "           [0.1686, 0.1647, 0.1647,  ..., 0.2078, 0.2118, 0.2157],\n",
       "           [0.1843, 0.1804, 0.1843,  ..., 0.2000, 0.2039, 0.2118]]],\n",
       " \n",
       " \n",
       "         [[[0.6039, 0.6078, 0.6157,  ..., 0.6314, 0.6471, 0.6353],\n",
       "           [0.6157, 0.6039, 0.6196,  ..., 0.6314, 0.6392, 0.6353],\n",
       "           [0.6157, 0.6039, 0.6196,  ..., 0.6314, 0.6392, 0.6431],\n",
       "           ...,\n",
       "           [0.8706, 0.8784, 0.8941,  ..., 0.9412, 0.9373, 0.9216],\n",
       "           [0.8824, 0.8824, 0.8941,  ..., 0.9490, 0.9373, 0.9294],\n",
       "           [0.8902, 0.8784, 0.8941,  ..., 0.9529, 0.9333, 0.9255]],\n",
       " \n",
       "          [[0.3804, 0.3882, 0.3961,  ..., 0.4471, 0.4667, 0.4549],\n",
       "           [0.3961, 0.3843, 0.4000,  ..., 0.4471, 0.4588, 0.4510],\n",
       "           [0.3922, 0.3843, 0.4000,  ..., 0.4471, 0.4549, 0.4588],\n",
       "           ...,\n",
       "           [0.8941, 0.8745, 0.8706,  ..., 0.8510, 0.8706, 0.8549],\n",
       "           [0.9059, 0.8784, 0.8824,  ..., 0.8549, 0.8627, 0.8667],\n",
       "           [0.9098, 0.8745, 0.8863,  ..., 0.8549, 0.8510, 0.8627]],\n",
       " \n",
       "          [[0.0863, 0.1020, 0.0980,  ..., 0.0314, 0.0353, 0.0353],\n",
       "           [0.0980, 0.0941, 0.0980,  ..., 0.0314, 0.0314, 0.0314],\n",
       "           [0.0941, 0.0941, 0.0980,  ..., 0.0314, 0.0392, 0.0392],\n",
       "           ...,\n",
       "           [0.8471, 0.8314, 0.8275,  ..., 0.7804, 0.7882, 0.7843],\n",
       "           [0.8745, 0.8510, 0.8549,  ..., 0.8000, 0.8039, 0.8196],\n",
       "           [0.8863, 0.8549, 0.8627,  ..., 0.8118, 0.8078, 0.8196]]],\n",
       " \n",
       " \n",
       "         [[[0.9725, 0.9529, 0.9529,  ..., 0.9529, 0.9569, 0.9569],\n",
       "           [0.9725, 0.9647, 0.9725,  ..., 0.9725, 0.9686, 0.9569],\n",
       "           [0.9765, 0.8667, 0.7686,  ..., 0.7255, 0.7804, 0.9412],\n",
       "           ...,\n",
       "           [0.9765, 0.8392, 0.6118,  ..., 0.4941, 0.4863, 0.6118],\n",
       "           [0.9647, 0.9490, 0.8275,  ..., 0.5843, 0.5882, 0.6824],\n",
       "           [0.9686, 0.9412, 0.8745,  ..., 0.7804, 0.7804, 0.8235]],\n",
       " \n",
       "          [[0.9725, 0.9569, 0.9608,  ..., 0.9647, 0.9608, 0.9569],\n",
       "           [0.9725, 0.9608, 0.9647,  ..., 0.9647, 0.9647, 0.9569],\n",
       "           [0.9765, 0.8549, 0.7412,  ..., 0.6980, 0.7608, 0.9373],\n",
       "           ...,\n",
       "           [0.9686, 0.8000, 0.5412,  ..., 0.4902, 0.4824, 0.6078],\n",
       "           [0.9647, 0.9333, 0.8000,  ..., 0.5843, 0.5882, 0.6824],\n",
       "           [0.9686, 0.9490, 0.8824,  ..., 0.7804, 0.7804, 0.8235]],\n",
       " \n",
       "          [[0.9804, 0.9529, 0.9451,  ..., 0.9373, 0.9412, 0.9490],\n",
       "           [0.9765, 0.9529, 0.9490,  ..., 0.9490, 0.9490, 0.9490],\n",
       "           [0.9725, 0.8431, 0.7294,  ..., 0.6863, 0.7529, 0.9333],\n",
       "           ...,\n",
       "           [0.9686, 0.7686, 0.4824,  ..., 0.4863, 0.4824, 0.6118],\n",
       "           [0.9608, 0.9216, 0.7765,  ..., 0.5843, 0.5882, 0.6824],\n",
       "           [0.9686, 0.9529, 0.8902,  ..., 0.7804, 0.7804, 0.8235]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.1922, 0.2039, 0.2157,  ..., 0.1490, 0.1412, 0.1686],\n",
       "           [0.1843, 0.2078, 0.2392,  ..., 0.1882, 0.1843, 0.2078],\n",
       "           [0.1765, 0.1961, 0.2353,  ..., 0.1961, 0.1843, 0.2353],\n",
       "           ...,\n",
       "           [0.2706, 0.2588, 0.3098,  ..., 0.0510, 0.0510, 0.0510],\n",
       "           [0.2706, 0.2667, 0.3373,  ..., 0.0863, 0.0706, 0.0627],\n",
       "           [0.2471, 0.3059, 0.3216,  ..., 0.1412, 0.1412, 0.1373]],\n",
       " \n",
       "          [[0.1686, 0.1804, 0.1922,  ..., 0.1333, 0.1255, 0.1529],\n",
       "           [0.1608, 0.1843, 0.2157,  ..., 0.1608, 0.1569, 0.1804],\n",
       "           [0.1529, 0.1725, 0.2118,  ..., 0.1647, 0.1529, 0.2039],\n",
       "           ...,\n",
       "           [0.2235, 0.2118, 0.2627,  ..., 0.0510, 0.0471, 0.0510],\n",
       "           [0.2235, 0.2196, 0.2902,  ..., 0.0824, 0.0627, 0.0588],\n",
       "           [0.2000, 0.2588, 0.2745,  ..., 0.1294, 0.1255, 0.1216]],\n",
       " \n",
       "          [[0.1686, 0.1804, 0.1922,  ..., 0.1294, 0.1216, 0.1490],\n",
       "           [0.1608, 0.1843, 0.2157,  ..., 0.1569, 0.1490, 0.1725],\n",
       "           [0.1529, 0.1725, 0.2118,  ..., 0.1608, 0.1529, 0.2039],\n",
       "           ...,\n",
       "           [0.2314, 0.2196, 0.2706,  ..., 0.0706, 0.0706, 0.0706],\n",
       "           [0.2314, 0.2275, 0.2941,  ..., 0.1020, 0.0863, 0.0784],\n",
       "           [0.2039, 0.2667, 0.2824,  ..., 0.1451, 0.1451, 0.1412]]],\n",
       " \n",
       " \n",
       "         [[[0.3961, 0.4000, 0.4078,  ..., 0.4314, 0.4196, 0.4039],\n",
       "           [0.4078, 0.4118, 0.4196,  ..., 0.4431, 0.4275, 0.4196],\n",
       "           [0.4157, 0.4196, 0.4275,  ..., 0.4431, 0.4275, 0.4196],\n",
       "           ...,\n",
       "           [0.2549, 0.2627, 0.2745,  ..., 0.2941, 0.2902, 0.2784],\n",
       "           [0.2510, 0.2627, 0.2627,  ..., 0.2941, 0.2902, 0.2745],\n",
       "           [0.2471, 0.2471, 0.2471,  ..., 0.2980, 0.2902, 0.2863]],\n",
       " \n",
       "          [[0.4941, 0.5020, 0.5059,  ..., 0.5176, 0.5020, 0.4902],\n",
       "           [0.5059, 0.5098, 0.5176,  ..., 0.5294, 0.5176, 0.5059],\n",
       "           [0.5059, 0.5098, 0.5176,  ..., 0.5294, 0.5176, 0.5098],\n",
       "           ...,\n",
       "           [0.2824, 0.2902, 0.3020,  ..., 0.3137, 0.3059, 0.2980],\n",
       "           [0.2784, 0.2902, 0.2902,  ..., 0.3098, 0.3059, 0.2941],\n",
       "           [0.2745, 0.2745, 0.2745,  ..., 0.3137, 0.3059, 0.3020]],\n",
       " \n",
       "          [[0.6431, 0.6431, 0.6510,  ..., 0.6667, 0.6549, 0.6431],\n",
       "           [0.6471, 0.6510, 0.6588,  ..., 0.6745, 0.6627, 0.6510],\n",
       "           [0.6431, 0.6431, 0.6510,  ..., 0.6667, 0.6549, 0.6431],\n",
       "           ...,\n",
       "           [0.3569, 0.3608, 0.3765,  ..., 0.3569, 0.3529, 0.3451],\n",
       "           [0.3569, 0.3647, 0.3647,  ..., 0.3569, 0.3529, 0.3412],\n",
       "           [0.3490, 0.3490, 0.3490,  ..., 0.3608, 0.3529, 0.3490]]],\n",
       " \n",
       " \n",
       "         [[[0.8784, 0.7294, 0.7451,  ..., 0.7686, 0.7804, 0.8902],\n",
       "           [0.7922, 0.4980, 0.5725,  ..., 0.6235, 0.6353, 0.8471],\n",
       "           [0.7569, 0.4667, 0.6078,  ..., 0.6706, 0.6706, 0.8627],\n",
       "           ...,\n",
       "           [0.7255, 0.3294, 0.3608,  ..., 0.2941, 0.3020, 0.7373],\n",
       "           [0.7412, 0.3059, 0.2980,  ..., 0.2667, 0.2863, 0.7373],\n",
       "           [0.8118, 0.5255, 0.4863,  ..., 0.4353, 0.4745, 0.8000]],\n",
       " \n",
       "          [[0.8667, 0.7020, 0.6902,  ..., 0.6980, 0.7176, 0.8745],\n",
       "           [0.7765, 0.4471, 0.4784,  ..., 0.5059, 0.5255, 0.8000],\n",
       "           [0.7333, 0.4118, 0.5137,  ..., 0.5490, 0.5451, 0.8078],\n",
       "           ...,\n",
       "           [0.7529, 0.3451, 0.3608,  ..., 0.2588, 0.2667, 0.7216],\n",
       "           [0.7529, 0.3176, 0.3020,  ..., 0.2431, 0.2588, 0.7216],\n",
       "           [0.8157, 0.5373, 0.4980,  ..., 0.4235, 0.4510, 0.7804]],\n",
       " \n",
       "          [[0.8196, 0.5961, 0.5647,  ..., 0.5608, 0.5922, 0.8078],\n",
       "           [0.7059, 0.2824, 0.2510,  ..., 0.2275, 0.2941, 0.6745],\n",
       "           [0.6745, 0.2392, 0.2627,  ..., 0.2431, 0.3020, 0.6902],\n",
       "           ...,\n",
       "           [0.6588, 0.2078, 0.2039,  ..., 0.1490, 0.1804, 0.6745],\n",
       "           [0.7020, 0.2157, 0.1608,  ..., 0.1373, 0.1686, 0.6588],\n",
       "           [0.7490, 0.4235, 0.3569,  ..., 0.3569, 0.3843, 0.7333]]]]),\n",
       " tensor([2, 6, 3, 5, 7, 6, 1, 8, 5, 4, 7, 3, 9, 9, 4, 5, 3, 0, 5, 3, 1, 3, 8, 9,\n",
       "         7, 4, 8, 5, 1, 2, 8, 4])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar(nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, output_size, shape):\n",
    "        super(Cifar, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_size, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*shape*shape, out_features=hidden_units*(shape+2)*(shape+2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units*(shape+2)*(shape+2), out_features=hidden_units*(shape+5)*(shape+5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units*(shape+5)*(shape+5), out_features=output_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cifar(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=250, out_features=490, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=490, out_features=1000, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Cifar(input_size=3, hidden_units=10, output_size=10, shape=5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "    model.train()\n",
    "    train_loss, accuracy = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        accuracy += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    accuracy = accuracy/len(dataloader)\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model: nn.Module,\n",
    "        loss_fn: nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "    model.eval()\n",
    "    test_loss, accuracy = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "            accuracy += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "    test_loss = test_loss/len(dataloader)\n",
    "    accuracy = accuracy/len(dataloader)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 1.9482 | Train Acc: 0.2633 | Test Loss: 1.7354 | Test Acc: 0.3503\n",
      "Epoch: 2 | Train Loss: 1.6890 | Train Acc: 0.3783 | Test Loss: 1.5947 | Test Acc: 0.4063\n",
      "Epoch: 3 | Train Loss: 1.5923 | Train Acc: 0.4202 | Test Loss: 1.4928 | Test Acc: 0.4491\n",
      "Epoch: 4 | Train Loss: 1.5292 | Train Acc: 0.4442 | Test Loss: 1.5287 | Test Acc: 0.4410\n",
      "Epoch: 5 | Train Loss: 1.4823 | Train Acc: 0.4619 | Test Loss: 1.3999 | Test Acc: 0.4856\n",
      "Epoch: 6 | Train Loss: 1.4456 | Train Acc: 0.4778 | Test Loss: 1.3994 | Test Acc: 0.4966\n",
      "Epoch: 7 | Train Loss: 1.4150 | Train Acc: 0.4864 | Test Loss: 1.3677 | Test Acc: 0.5068\n",
      "Epoch: 8 | Train Loss: 1.3960 | Train Acc: 0.4955 | Test Loss: 1.3107 | Test Acc: 0.5263\n",
      "Epoch: 9 | Train Loss: 1.3677 | Train Acc: 0.5063 | Test Loss: 1.2927 | Test Acc: 0.5349\n",
      "Epoch: 10 | Train Loss: 1.3452 | Train Acc: 0.5164 | Test Loss: 1.2911 | Test Acc: 0.5322\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_accuracy\": []\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model=model,\n",
    "                                       dataloader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer)\n",
    "    test_loss, test_acc = test(model=model,\n",
    "                                    dataloader=val_dataloader,\n",
    "                                    loss_fn=loss_fn)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_accuracy\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_accuracy\"].append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2997 | Test Acc: 0.5319\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test(model=model,\n",
    "                                    dataloader=test_dataloader,\n",
    "                                    loss_fn=loss_fn)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
